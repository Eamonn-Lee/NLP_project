{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in /Users/garry/vs_code/6713/venv/lib/python3.11/site-packages (6.30.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "%pip install protobuf\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../CODE-Baseline\")  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from salary_baseline import extract_salary_with_inference\n",
    "\n",
    "\n",
    "file_path = '../DATASETS/salary_labelled_development_set.csv'\n",
    "test_file_path = '../DATASETS/salary_labelled_test_set.csv'\n",
    "ignore_id_path = '../DATASETS/err_salary_develpment.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df_ignore = pd.read_csv(ignore_id_path)\n",
    "# some y_true is impossible\n",
    "df = df[~df['job_id'].isin(df_ignore['job_id'])]\n",
    "\n",
    "\n",
    "model_name = \"google/mt5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理 HTML 标签\n",
    "def clean_html_tags(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    return soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# 构造输入输出\n",
    "def row_to_input_output(row):\n",
    "    context = clean_html_tags(f\"{row['job_title']} {row['job_ad_details']}\")\n",
    "    prompt = (\n",
    "        \"Extract the salary info from the job ad below and return it as: \"\n",
    "        \"\\\"MinimumSalary MaximumSalary Currency PayPeriod\\\"\\n\\n\"\n",
    "    )\n",
    "    input_text = prompt + context\n",
    "    output_text = str(row[\"y_true\"]).strip().replace(\"-\", \" \")\n",
    "    return {\"input\": input_text, \"output\": output_text}\n",
    "\n",
    "# 预处理函数\n",
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(example[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"output\"], max_length=64, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    labels = [token if token != tokenizer.pad_token_id else -100 for token in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8460254d6714b2cbf19b4a84ee9fa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size： 1706\n",
      "testing data size： 427\n"
     ]
    }
   ],
   "source": [
    "# tokenized_dataset = squad_dataset\n",
    "squad_dataset = Dataset.from_list([row_to_input_output(row) for _, row in df.iterrows()])\n",
    "tokenized_dataset = squad_dataset.map(preprocess)\n",
    "final_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(\"training data size：\", len(final_dataset['train']))\n",
    "print(\"testing data size：\", len(final_dataset['test']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看训练集中的两个样本\n",
    "# for i in range(10):\n",
    "#     print(f\"\\nExample {i+1}:\")\n",
    "#     print(\"Input:\", final_dataset[\"train\"][i])\n",
    "#     print(\"Input:\", final_dataset[\"train\"][i][\"input\"])\n",
    "#     print(\"Output:\", final_dataset[\"train\"][i][\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10249' max='14945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10249/14945 3:38:04 < 1:39:56, 0.78 it/s, Epoch 24/35]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.949920</td>\n",
       "      <td>0.214156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.323700</td>\n",
       "      <td>0.330867</td>\n",
       "      <td>0.598184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.059200</td>\n",
       "      <td>0.285910</td>\n",
       "      <td>0.627384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.237781</td>\n",
       "      <td>0.649855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.484400</td>\n",
       "      <td>0.231004</td>\n",
       "      <td>0.650859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.239855</td>\n",
       "      <td>0.654734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.261861</td>\n",
       "      <td>0.648192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.319200</td>\n",
       "      <td>0.221385</td>\n",
       "      <td>0.658080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.259400</td>\n",
       "      <td>0.208767</td>\n",
       "      <td>0.662763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.188928</td>\n",
       "      <td>0.659836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.198564</td>\n",
       "      <td>0.658080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>0.175288</td>\n",
       "      <td>0.663349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.192858</td>\n",
       "      <td>0.663349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.193668</td>\n",
       "      <td>0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.171600</td>\n",
       "      <td>0.183729</td>\n",
       "      <td>0.665691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.182464</td>\n",
       "      <td>0.668033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.175758</td>\n",
       "      <td>0.665105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.189164</td>\n",
       "      <td>0.664520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.120900</td>\n",
       "      <td>0.185935</td>\n",
       "      <td>0.669789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.177763</td>\n",
       "      <td>0.667447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.184936</td>\n",
       "      <td>0.666862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.184930</td>\n",
       "      <td>0.668033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.198240</td>\n",
       "      <td>0.668033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.184416</td>\n",
       "      <td>0.668033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     35\u001b[39m training_args = Seq2SeqTrainingArguments(\n\u001b[32m     36\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./mt5-base-salary\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m     eval_strategy=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     greater_is_better=\u001b[38;5;28;01mTrue\u001b[39;00m,  \n\u001b[32m     50\u001b[39m )\n\u001b[32m     53\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     54\u001b[39m     model=model,\n\u001b[32m     55\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     compute_metrics=compute_metrics  \n\u001b[32m     60\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/transformers/trainer.py:2661\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2658\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2660\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2661\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2667\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/transformers/trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/transformers/trainer.py:3200\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3198\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3199\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3203\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/transformers/trainer.py:3902\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3899\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3901\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3902\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3904\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/transformers/trainer.py:4006\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   4004\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   4005\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4006\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4007\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   4008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4010\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4011\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3564\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3559\u001b[39m     gc.collect()\n\u001b[32m   3561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3562\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3563\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3564\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3565\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3566\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vs_code/6713/venv/lib/python3.11/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 28, kind: StorageFull, message: \"No space left on device\" })"
     ]
    }
   ],
   "source": [
    "# print(transformers.__version__)\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "\n",
    "    # 有些模型的 preds 是 tuple，取第一个元素\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # 转成 list 并移除 -100\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # 对于 labels，要先把 -100 替换为 pad_token_id，防止 decode 出错\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # 计算 F1 分数\n",
    "    f1_scores = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        pred_tokens = word_tokenize(pred.lower())\n",
    "        label_tokens = word_tokenize(label.lower())\n",
    "\n",
    "        common = set(pred_tokens) & set(label_tokens)\n",
    "        if len(pred_tokens) == 0 or len(label_tokens) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            precision = len(common) / len(pred_tokens)\n",
    "            recall = len(common) / len(label_tokens)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return {\n",
    "        \"f1\": np.mean(f1_scores)\n",
    "    }\n",
    "    \n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mt5-base-salary\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=35,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    predict_with_generate=True,\n",
    "    # save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"f1\",  \n",
    "    greater_is_better=True,  \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./mt5-base-salary\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"./mt5-base-salary\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to('cpu')\n",
    "qa_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "test_samples = [final_dataset[\"test\"][i] for i in range(100)]\n",
    "\n",
    "# # 生成并打印结果\n",
    "for i, example in enumerate(test_samples):\n",
    "    sample = example['input']\n",
    "    print(f\"Test Sample {i + 1}: {sample}\")\n",
    "    output = qa_pipeline(sample)\n",
    "    print(f\"Prediction: {output[0]['generated_text']}\")\n",
    "    print(f\"Answer: {example['output']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
