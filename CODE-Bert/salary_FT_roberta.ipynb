{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development dataset:\n",
      "Precision: 0.8357\n",
      "Recall: 0.9524\n",
      "F1 Score: 0.8902\n",
      "Accuracy: 0.8889\n",
      "\n",
      "Test dataset:\n",
      "Precision: 0.7412\n",
      "Recall: 0.9206\n",
      "F1 Score: 0.8212\n",
      "Accuracy: 0.8219\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import json\n",
    "import evaluate\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "sys.path.append(\"../CODE-Baseline\")  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from salary_baseline import infer_period_by_amount\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "country_currency_map = {\n",
    "    \"PH\": \"PHP\", \"AUS\": \"AUD\", \"NZ\": \"NZD\", \"SG\": \"SGD\",\n",
    "    \"MY\": \"MYR\", \"TH\": \"THB\", \"ID\": \"IDR\", \"HK\": \"HKD\"\n",
    "}\n",
    "salary_keywords=['å¾…é‡', 'salary', 'wage', 'compensation', 'remuneration', 'gaji', 'bermula', 'basic', 'pokok',\n",
    "                      'income']\n",
    "def convert_k_to_number(text):\n",
    "    # åŒ¹é…å½¢å¦‚ 20kã€16.5kã€30K çš„æ•°å­—\n",
    "    def replace(match):\n",
    "        num = float(match.group(1))\n",
    "        return str(int(num * 1000))\n",
    "\n",
    "    return re.sub(r'(\\d+(?:\\.\\d+)?)k', replace, text, flags=re.IGNORECASE)\n",
    "# html -> text\n",
    "def clean_html_tags(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    \n",
    "    # formate text\n",
    "    text = re.sub(r\"[â€¢â—â–ªâ–ºâ—†â˜…â™¦âœ“âœ”â¬¤â–]\", \"\", text)\n",
    "    text = text.replace(\",\", \"\")\n",
    "    for _, value in country_currency_map.items():\n",
    "      text = text.replace(value, \"$\")\n",
    "    text = text.replace(\"RM\", \"$\")\n",
    "    text = text.replace(\"à¸¿\", \"$\")\n",
    "    text = text.replace(\"AU\", \"$\")\n",
    "    text = text.replace(\"$$\", \"$\")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = re.sub(r'\\b[Tt][Oo]\\b', '-', text)\n",
    "    text = text.replace(\"and\", \"-\")\n",
    "    text = text.replace(\"&\", \"-\")\n",
    "    text = text.replace(\"è‡³\", \"-\")\n",
    "    text = text.replace(\"hingga ke\", \"-\")\n",
    "    text = text.replace(\"hingga\", \"-\")\n",
    "    text = text.replace(\"Hingga\", \"-\")\n",
    "    text = text.replace(\"HINGGA\", \"-\")\n",
    "    text = convert_k_to_number(text)\n",
    "    # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼Œ\\bè¡¨ç¤ºå•è¯è¾¹ç•Œï¼Œ|è¡¨ç¤ºâ€œæˆ–â€\n",
    "    pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in salary_keywords) + r')\\b'\n",
    "\n",
    "    # æ›¿æ¢ä¸º compensationï¼Œflags=re.IGNORECASE è¡¨ç¤ºä¸åŒºåˆ†å¤§å°å†™\n",
    "    text = re.sub(pattern, 'compensation', text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "{\n",
    "  \"context\": \"Financial Account - Call Center Agent - Up - 34k\\n...ï¼ˆçœç•¥ï¼‰...\",\n",
    "  \"question\": \"what is fixed compensation\",\n",
    "  \"answers\": {\n",
    "    \"text\": [\"17500\"],\n",
    "    \"answer_start\": [1257]\n",
    "  },\n",
    "  \"is_impossible\": false\n",
    "}\n",
    "'''\n",
    "# from labelled json -> Squad2 json \n",
    "with open('../DATASETS/1900_labelled.json', \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "    \n",
    "processed = []\n",
    "for item in raw_data:\n",
    "    context = item[\"raw_text\"]\n",
    "\n",
    "    # fixed salary\n",
    "    if \"answer_1\" in item and item[\"answer_1\"]:\n",
    "        processed.append({\n",
    "            \"context\": context,\n",
    "            \"question\": \"what is fixed compensation?\",\n",
    "            \"answers\": {\n",
    "                \"text\": [item[\"answer_1\"][0][\"text\"]],\n",
    "                \"answer_start\": [item[\"answer_1\"][0][\"start\"]]\n",
    "            },\n",
    "            \"is_impossible\": False\n",
    "        })\n",
    "    else:\n",
    "        processed.append({\n",
    "            \"context\": context,\n",
    "            \"question\": \"what is fixed compensation?\",\n",
    "            \"answers\": {\n",
    "                \"text\": [],\n",
    "                \"answer_start\": []\n",
    "            },\n",
    "            \"is_impossible\": True\n",
    "        })\n",
    "\n",
    "    # salary range\n",
    "    if \"answer_2\" in item and item[\"answer_2\"]:\n",
    "        processed.append({\n",
    "            \"context\": context,\n",
    "            \"question\": \"what is compensation range?\",\n",
    "            \"answers\": {\n",
    "                \"text\": [item[\"answer_2\"][0][\"text\"]],\n",
    "                \"answer_start\": [item[\"answer_2\"][0][\"start\"]]\n",
    "            },\n",
    "            \"is_impossible\": False\n",
    "        })\n",
    "    else:\n",
    "        processed.append({\n",
    "            \"context\": context,\n",
    "            \"question\": \"what is compensation range?\",\n",
    "            \"answers\": {\n",
    "                \"text\": [],\n",
    "                \"answer_start\": []\n",
    "            },\n",
    "            \"is_impossible\": True\n",
    "        })\n",
    "\n",
    "    # pay period\n",
    "    if \"answer_3\" in item and item[\"answer_3\"]:\n",
    "        processed.append({\n",
    "            \"context\": context,\n",
    "            \"question\": \"what is pay period?\",\n",
    "            \"answers\": {\n",
    "                \"text\": [item[\"answer_3\"][0][\"text\"]],\n",
    "                \"answer_start\": [item[\"answer_3\"][0][\"start\"]]\n",
    "            },\n",
    "            \"is_impossible\": False\n",
    "        })\n",
    "    else:\n",
    "        processed.append({\n",
    "            \"context\": context,\n",
    "            \"question\": \"what is pay period?\",\n",
    "            \"answers\": {\n",
    "                \"text\": [],\n",
    "                \"answer_start\": []\n",
    "            },\n",
    "            \"is_impossible\": True\n",
    "        })\n",
    "\n",
    "\n",
    "with open(\"qa_dataset_squad2-1900.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "with open(\"qa_dataset_squad2-1900.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# Step 2: è½¬æ¢ä¸º HuggingFace Dataset\n",
    "dataset = Dataset.from_list(squad_data)\n",
    "dataset = dataset.train_test_split(test_size=0.1,seed=38)  \n",
    "\n",
    "# Step 3: Tokenizer å¤„ç†å‡½æ•°ï¼ˆåŒ…å« offset æ˜ å°„ï¼‰\n",
    "def preprocess(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=512,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = example[\"answers\"][sample_index]\n",
    "        if example[\"is_impossible\"][sample_index]:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # æ‰¾åˆ° token çš„ start & end\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # æ‰¾åˆ°å®é™…çš„ token span\n",
    "            start_idx = cls_index\n",
    "            end_idx = cls_index\n",
    "            for idx in range(token_start_index, token_end_index + 1):\n",
    "                if offsets[idx] is None:\n",
    "                    continue\n",
    "                start, end = offsets[idx]\n",
    "                if start <= start_char < end:\n",
    "                    start_idx = idx\n",
    "                if start < end_char <= end:\n",
    "                    end_idx = idx\n",
    "                    break\n",
    "\n",
    "            start_positions.append(start_idx)\n",
    "            end_positions.append(end_idx)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized\n",
    "\n",
    "# Step 4: é¢„å¤„ç†æ•°æ®\n",
    "tokenized_datasets = dataset.map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    # å°†é¢„æµ‹å’Œæ ‡ç­¾è½¬æ¢ä¸º Tensor\n",
    "    start_preds = torch.tensor(predictions[0])\n",
    "    end_preds = torch.tensor(predictions[1])\n",
    "    start_labels = torch.tensor(labels[0])\n",
    "    end_labels = torch.tensor(labels[1])\n",
    "    \n",
    "    # è·å– start å’Œ end çš„é¢„æµ‹ä½ç½®\n",
    "    start_pred = torch.argmax(start_preds, dim=1)\n",
    "    end_pred = torch.argmax(end_preds, dim=1)\n",
    "    \n",
    "    # è®¡ç®—æŸå¤±\n",
    "    start_loss = torch.nn.CrossEntropyLoss()(start_preds, start_labels)\n",
    "    end_loss = torch.nn.CrossEntropyLoss()(end_preds, end_labels)\n",
    "    total_loss = start_loss + end_loss\n",
    "    \n",
    "    # è®¡ç®— exact match\n",
    "    exact_match = (start_pred == start_labels).float().mean() * (end_pred == end_labels).float().mean()\n",
    "\n",
    "    # F1 score\n",
    "    # å°† start_pred, start_labels, end_pred, end_labels è½¬æ¢ä¸º numpy æ•°ç»„\n",
    "    start_pred_np = start_pred.cpu().numpy()\n",
    "    start_labels_np = start_labels.cpu().numpy()\n",
    "    end_pred_np = end_pred.cpu().numpy()\n",
    "    end_labels_np = end_labels.cpu().numpy()\n",
    "\n",
    "    # è®¡ç®— F1 score\n",
    "    f1_start = f1_score(start_labels_np, start_pred_np, average=\"micro\")\n",
    "    f1_end = f1_score(end_labels_np, end_pred_np, average=\"micro\")\n",
    "\n",
    "    # è®¡ç®—æ€»çš„ F1 scoreï¼ˆå¯ä»¥æ ¹æ®éœ€æ±‚è‡ªå·±å†³å®šå¦‚ä½•ç»“åˆ start å’Œ end çš„ F1 åˆ†æ•°ï¼‰\n",
    "    f1 = (f1_start + f1_end) / 2\n",
    "\n",
    "    return {\n",
    "        \"start_loss\": start_loss.item(),\n",
    "        \"end_loss\": end_loss.item(),\n",
    "        \"total_loss\": total_loss.item(),\n",
    "        \"exact_match\": exact_match.item(),\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "  \n",
    "# Step 5: è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta-qa\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"f1\",  \n",
    "    greater_is_better=True,  \n",
    ")\n",
    "\n",
    "# Step 6: Trainer åˆå§‹åŒ–\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# Step 7: å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./roberta-qa-1900\")\n",
    "tokenizer.save_pretrained(\"./roberta-qa-1900\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_fixed_salary(text):\n",
    "    text = re.sub(r'[^\\d\\-.]', '', text)\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?)\\s*'  # åŒ¹é…æ•°å­—æˆ–å°æ•°\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        salary = round(float(match.group(1)))\n",
    "        return salary\n",
    "    return None  \n",
    "  \n",
    "def get_salary_range(text):\n",
    "    text = text.replace(\"\\n\", \"-\")\n",
    "    text = re.sub(r'[^\\d\\-.]', '', text)\n",
    "    pattern = r'(\\d+(?:\\.\\d+)?)(?:[-â€“â€”]+)(\\d+(?:\\.\\d+)?)?'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        min_salary = match.group(1)\n",
    "        if match.group(2):\n",
    "            is_range = True\n",
    "        else:\n",
    "          return None\n",
    "        max_salary = match.group(2) if match.group(2) else match.group(1)\n",
    "\n",
    "        min_salary = round(float(min_salary))\n",
    "        max_salary = round(float(max_salary))\n",
    "\n",
    "        if min_salary > max_salary:\n",
    "            return None\n",
    "        return (min_salary,max_salary)\n",
    "    return None \n",
    "\n",
    "import re\n",
    "\n",
    "def get_period(text):\n",
    "    unit_patterns = {\n",
    "        \"HOURLY\": r'(per\\s*hour|hourly|hr\\b|/hr\\b|/hour\\b|æ™‚è–ª|æ¯å°æ™‚|æ¯å°æ™‚è–ªè³‡|æ¯ç¯€)',\n",
    "        \"DAILY\": r'(per\\s*day|daily|/day\\b|æ—¥è–ª|æ¯å¤©|æ¯æ—¥è–ªè³‡)',\n",
    "        \"WEEKLY\": r'(per\\s*week|weekly|/week\\b|é€±è–ª|æ¯é€±|æ¯å‘¨è–ªè³‡|å‘¨è–ª)',\n",
    "        \"MONTHLY\": r'(per\\s*month|monthly|/month\\b|/Mth\\b|æœˆè–ª|æ¯æœˆ|æ¯æœˆè–ªè³‡|sebulan|bulanan)',\n",
    "        \"ANNUAL\": r'(per\\s*year|yearly|annually|remuneration|super|annum|p\\.a\\.|p/a|/year\\b|å¹´è–ª|æ¯å¹´|å¹´åº¦è–ªè³‡)'\n",
    "    }\n",
    "\n",
    "    text = text.lower()\n",
    "    for period, pattern in unit_patterns.items():\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return period\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ä½  fine-tuned çš„æ¨¡å‹å’Œ tokenizer\n",
    "model_path = \"./roberta-qa-1900-seed48\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# æ„é€  pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "questions = [\n",
    "    \"What is fixed compensation?\",\n",
    "    \"What is compensation range?\",\n",
    "    \"What is pay period?\"\n",
    "]\n",
    "file_path = '../DATASETS/salary_labelled_development_set.csv'\n",
    "test_file_path = '../DATASETS/salary_labelled_test_set.csv'\n",
    "\n",
    "country_currency_map = {\n",
    "    \"PH\": \"PHP\", \"AUS\": \"AUD\", \"NZ\": \"NZD\", \"SG\": \"SGD\",\n",
    "    \"MY\": \"MYR\", \"TH\": \"THB\", \"ID\": \"IDR\", \"HK\": \"HKD\"\n",
    "}\n",
    "\n",
    "def get_salary_using_FT_RoBerta(text, nation_code=None):\n",
    "    context = clean_html_tags(text)\n",
    "    currency = country_currency_map.get(nation_code, \"None\")\n",
    "    questions = [\n",
    "        \"What is fixed compensation?\",\n",
    "        \"What is compensation range?\",\n",
    "        \"What is pay period?\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for q in questions:\n",
    "        output = qa_pipeline(question=q, context=context)\n",
    "        answer = output['answer']\n",
    "        score = output['score']\n",
    "        results.append((q, answer, score))\n",
    "\n",
    "\n",
    "    fixed_salary = get_fixed_salary(results[0][1])\n",
    "    salary_range = get_salary_range(results[1][1])\n",
    "    if salary_range:\n",
    "        min_salary, max_salary = salary_range\n",
    "    else:\n",
    "        min_salary, max_salary = None, None\n",
    "    period = get_period(results[2][1])\n",
    "    # period = None\n",
    "    if period is None and (salary_range or fixed_salary) :\n",
    "      if fixed_salary:\n",
    "        if salary_range:\n",
    "          if min_salary < fixed_salary < max_salary:\n",
    "            avg = fixed_salary\n",
    "          else:\n",
    "            avg = (int(min_salary) + int(max_salary)) / 2\n",
    "        else:\n",
    "          avg = fixed_salary\n",
    "      else:\n",
    "        if salary_range:\n",
    "          avg = (int(min_salary) + int(max_salary)) / 2\n",
    "      period = infer_period_by_amount(avg, currency)\n",
    "\n",
    "\n",
    "\n",
    "    if fixed_salary:\n",
    "      if salary_range:\n",
    "        if min_salary < fixed_salary < max_salary:\n",
    "          return f\"{fixed_salary}-{fixed_salary}-{currency}-{period}\"\n",
    "        return f\"{min_salary}-{max_salary}-{currency}-{period}\"\n",
    "      else:\n",
    "        return f\"{fixed_salary}-{fixed_salary}-{currency}-{period}\"\n",
    "    else:\n",
    "      if salary_range:\n",
    "        return f\"{min_salary}-{max_salary}-{currency}-{period}\"\n",
    "      else:\n",
    "        return \"0-0-None-None\"\n",
    "    return \"0-0-None-None\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_equal(predict, y_true):\n",
    "    pattern = r\"(\\d+)-(\\d+)-([A-Z]+)-([A-Z]+)\"\n",
    "\n",
    "    if predict==\"0-0-None-None\" and y_true==\"0-0-None-None\":\n",
    "      return True\n",
    "    match_pred = re.match(pattern, predict)\n",
    "    match_true = re.match(pattern, y_true)\n",
    "\n",
    "    if not match_pred or not match_true:\n",
    "      return False  \n",
    "\n",
    "    min_pred, max_pred, currency_pred, period_pred = match_pred.groups()\n",
    "    min_true, max_true, currency_true, period_true = match_true.groups()\n",
    "\n",
    "    # è½¬ä¸ºæ•´æ•°\n",
    "    min_pred, max_pred = int(min_pred), int(max_pred)\n",
    "    min_true, max_true = int(min_true), int(max_true)\n",
    "\n",
    "    # å…è®¸1çš„è¯¯å·®\n",
    "    min_ok = abs(min_pred - min_true) <= 1\n",
    "    max_ok = abs(max_pred - max_true) <= 1\n",
    "\n",
    "    # è´§å¸å’Œå‘¨æœŸå¿…é¡»ä¸¥æ ¼ç›¸ç­‰\n",
    "    currency_ok = currency_pred == currency_true\n",
    "    period_ok = period_pred == period_true\n",
    "\n",
    "    return min_ok and max_ok and currency_ok and period_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development dataset:\n",
      "Precision: 0.8266\n",
      "Recall: 0.9784\n",
      "F1 Score: 0.8961\n",
      "Accuracy: 0.8888\n",
      "\n",
      "Development dataset:\n",
      "Precision: 0.8899\n",
      "Recall: 0.9944\n",
      "F1 Score: 0.9393\n",
      "Accuracy: 0.9353\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "    \n",
    "    \n",
    "df['predicted_salary'] = df.apply(\n",
    "    lambda row: get_salary_using_FT_RoBerta(\n",
    "        f\"{row['job_title']} {row['job_ad_details']}\",\n",
    "        row['nation_short_desc']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# example = df.iloc[2]\n",
    "# predicted_salary = get_salary_using_FT_RoBerta(example['job_ad_details'],example['nation_short_desc'])\n",
    "\n",
    "\n",
    "# TP, FP, TN, FN\n",
    "TP = np.sum((df['predicted_salary'] == df['y_true']) & (df['y_true'] != \"0-0-None-None\"))\n",
    "FP = np.sum((df['predicted_salary'] != df['y_true']) & (df['predicted_salary'] != \"0-0-None-None\"))\n",
    "FN = np.sum((df['predicted_salary'] == \"0-0-None-None\") & (df['y_true'] != \"0-0-None-None\"))\n",
    "TN = np.sum((df['predicted_salary'] == \"0-0-None-None\") & (df['y_true'] == \"0-0-None-None\"))\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (TP + TN) / (FP + FN + TP + TN)\n",
    "\n",
    "# # Print prediction vs ground truth\n",
    "# print(\"\\nğŸ” Prediction vs Ground Truth:\\n\")\n",
    "# for i, row in df.iterrows():\n",
    "#     predicted = row['predicted_salary']\n",
    "#     expected = row['y_true']\n",
    "#     if predicted != expected:\n",
    "#         print(f\"[{i}] âŒ Predicted: {predicted} | Expected: {expected}\")\n",
    "#     else:\n",
    "#         print(f\"[{i}] âœ… Matched:   {predicted}\")\n",
    "\n",
    "print(\"Development dataset:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print()\n",
    "\n",
    "'''\n",
    "roberta-qa-1145 seed1145\n",
    "Development dataset:\n",
    "Precision: 0.7799\n",
    "Recall: 0.9545\n",
    "F1 Score: 0.8584\n",
    "Accuracy: 0.8474\n",
    "\n",
    "roberta-qa-1463 seed 1463\n",
    "Development dataset:\n",
    "Precision: 0.7284\n",
    "Recall: 0.9797\n",
    "F1 Score: 0.8356\n",
    "Accuracy: 0.8156\n",
    "\n",
    "roberta-qa-1463 seed 1611\n",
    "Development dataset:\n",
    "Precision: 0.8161\n",
    "Recall: 0.9586\n",
    "F1 Score: 0.8816\n",
    "Accuracy: 0.8738\n",
    "\n",
    "roberta-qa-1900 seed 2000 \n",
    "Development dataset:\n",
    "Precision: 0.7982\n",
    "Recall: 0.9696\n",
    "F1 Score: 0.8756\n",
    "Accuracy: 0.8641\n",
    "\n",
    "roberta-qa-1900 seed 48\n",
    "Development dataset:\n",
    "Precision: 0.8266\n",
    "Recall: 0.9784\n",
    "F1 Score: 0.8961\n",
    "Accuracy: 0.8888\n",
    "'''\n",
    "\n",
    "# the id whose y_true is wrong\n",
    "ignore_id_path = '../DATASETS/err_salary_develpment.csv'\n",
    "df_ignore = pd.read_csv(ignore_id_path)\n",
    "# some y_true is impossible\n",
    "df = df[~df['job_id'].isin(df_ignore['job_id'])]\n",
    "df['is_positive'] = df['predicted_salary'] != \"0-0-None-None\"\n",
    "# fuzzy åŒ¹é…\n",
    "df['is_fuzzy_match'] = df.apply(lambda row: fuzzy_equal(row['predicted_salary'], row['y_true']), axis=1)\n",
    "\n",
    "# è®¡ç®— TP / FP / FN / TN\n",
    "TP = np.sum(df['is_fuzzy_match'] & df['is_positive'])\n",
    "FP = np.sum(~df['is_fuzzy_match'] & df['is_positive'])\n",
    "FN = np.sum(~df['is_fuzzy_match'] & ~df['is_positive'])\n",
    "TN = np.sum(df['is_fuzzy_match'] & ~df['is_positive'])\n",
    "\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (TP + TN) / (FP + FN + TP + TN)\n",
    "\n",
    "print(\"Development dataset:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset:\n",
      "Precision: 0.7364\n",
      "Recall: 0.9643\n",
      "F1 Score: 0.8351\n",
      "Accuracy: 0.8307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nroberta-qa-1145 seed 1145\\nTest dataset:\\nPrecision: 0.7599\\nRecall: 0.9294\\nF1 Score: 0.8361\\nAccuracy: 0.8272\\n\\nroberta-qa-1463 seed 1145\\nTest dataset:\\nPrecision: 0.7265\\nRecall: 0.9623\\nF1 Score: 0.8279\\nAccuracy: 0.8131\\n\\nroberta-qa-1463 seed 1611\\nTest dataset:\\nPrecision: 0.7636\\nRecall: 0.9582\\nF1 Score: 0.8499\\nAccuracy: 0.8430\\n\\nroberta-qa-1900 seed 2000 \\nTest dataset:\\nPrecision: 0.7529\\nRecall: 0.9774\\nF1 Score: 0.8506\\nAccuracy: 0.8395\\n\\nroberta-qa-1900 seed 48\\nTest dataset:\\nPrecision: 0.7879\\nRecall: 0.9665\\nF1 Score: 0.8681\\nAccuracy: 0.8607\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test set\n",
    "df = pd.read_csv(test_file_path)\n",
    "\n",
    "# Apply extractor\n",
    "df['predicted_salary'] = df.apply(\n",
    "    lambda row: get_salary_using_FT_RoBerta(\n",
    "        f\"{row['job_title']} {row['job_ad_details']}\",\n",
    "        row['nation_short_desc']\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "TP = np.sum((df['predicted_salary'] == df['y_true']) & (df['y_true'] != \"0-0-None-None\"))\n",
    "FP = np.sum((df['predicted_salary'] != df['y_true']) & (df['predicted_salary'] != \"0-0-None-None\"))\n",
    "FN = np.sum((df['predicted_salary'] == \"0-0-None-None\") & (df['y_true'] != \"0-0-None-None\"))\n",
    "TN = np.sum((df['predicted_salary'] == \"0-0-None-None\") & (df['y_true'] == \"0-0-None-None\"))\n",
    "\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (TP + TN) / (FP + FN + TP + TN)\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "'''\n",
    "roberta-qa-1145 seed 1145\n",
    "Test dataset:\n",
    "Precision: 0.7599\n",
    "Recall: 0.9294\n",
    "F1 Score: 0.8361\n",
    "Accuracy: 0.8272\n",
    "\n",
    "roberta-qa-1463 seed 1145\n",
    "Test dataset:\n",
    "Precision: 0.7265\n",
    "Recall: 0.9623\n",
    "F1 Score: 0.8279\n",
    "Accuracy: 0.8131\n",
    "\n",
    "roberta-qa-1463 seed 1611\n",
    "Test dataset:\n",
    "Precision: 0.7636\n",
    "Recall: 0.9582\n",
    "F1 Score: 0.8499\n",
    "Accuracy: 0.8430\n",
    "\n",
    "roberta-qa-1900 seed 2000 \n",
    "Test dataset:\n",
    "Precision: 0.7529\n",
    "Recall: 0.9774\n",
    "F1 Score: 0.8506\n",
    "Accuracy: 0.8395\n",
    "\n",
    "roberta-qa-1900 seed 48\n",
    "Test dataset:\n",
    "Precision: 0.7879\n",
    "Recall: 0.9665\n",
    "F1 Score: 0.8681\n",
    "Accuracy: 0.8607\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print prediction vs ground truth\n",
    "# print(\"\\nğŸ” Prediction vs Ground Truth:\\n\")\n",
    "# for i, row in df.iterrows():\n",
    "#     predicted = row['predicted_salary']\n",
    "#     expected = row['y_true']\n",
    "#     if predicted != expected:\n",
    "#         print(f\"[{i}] âŒ Predicted: {predicted} | Expected: {expected}\")\n",
    "#     else:\n",
    "#         print(f\"[{i}] âœ… Matched:   {predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_positive'] = df['predicted_salary'] != \"0-0-None-None\"\n",
    "\n",
    "# fuzzy åŒ¹é…\n",
    "df['is_fuzzy_match'] = df.apply(lambda row: fuzzy_equal(row['predicted_salary'], row['y_true']), axis=1)\n",
    "\n",
    "# è®¡ç®— TP / FP / FN / TN\n",
    "TP = np.sum(df['is_fuzzy_match'] & df['is_positive'])\n",
    "FP = np.sum(~df['is_fuzzy_match'] & df['is_positive'])\n",
    "FN = np.sum(~df['is_fuzzy_match'] & ~df['is_positive'])\n",
    "TN = np.sum(df['is_fuzzy_match'] & ~df['is_positive'])\n",
    "\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "accuracy = (TP + TN) / (FP + FN + TP + TN)\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
