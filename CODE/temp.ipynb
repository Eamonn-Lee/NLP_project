{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e180291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Embeddings cached.\n",
      "Epoch 1, Loss: 5054.3152\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# === Constants ===\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<END>\"\n",
    "LABELS = [\"B-SALARY\", \"I-SALARY\", \"O\"]\n",
    "CACHE_FILE = \"cached_train_embeddings.pt\"\n",
    "\n",
    "# === Helper Functions ===\n",
    "def parse_actual_info(info_str):\n",
    "    parts = info_str.split('-')\n",
    "    if len(parts) != 4 or parts == ['0', '0', 'None', 'None']:\n",
    "        return None\n",
    "    return (float(parts[0]), float(parts[1]), parts[2], parts[3].lower())\n",
    "\n",
    "def clean_html_tags(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    cleaned = re.sub(r'<[^>]+>', '', text)           \n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()   \n",
    "    return cleaned\n",
    "\n",
    "def chunk_and_align(text, min_salary, max_salary, tokenizer, max_length=512, stride=128):\n",
    "    tokens_all, labels_all, embeddings_all = [], [], []\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    overflow_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mappings = inputs.pop(\"offset_mapping\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    for i in range(len(input_ids)):\n",
    "        chunk_offsets = offset_mappings[i].tolist()\n",
    "        chunk_input_ids = input_ids[i]\n",
    "        chunk_tokens = tokenizer.convert_ids_to_tokens(chunk_input_ids)\n",
    "        word_ids = inputs.word_ids(i)\n",
    "\n",
    "        labels = []\n",
    "        for token, offset, word_id in zip(chunk_tokens, chunk_offsets, word_ids):\n",
    "            if word_id is None or offset == [0, 0]:\n",
    "                labels.append(\"O\")\n",
    "                continue\n",
    "            word = text[offset[0]:offset[1]]\n",
    "            try:\n",
    "                value = float(re.sub(r'[^\\d.]', '', word))\n",
    "                if min_salary <= value <= max_salary:\n",
    "                    if labels and labels[-1] in [\"B-SALARY\", \"I-SALARY\"]:\n",
    "                        labels.append(\"I-SALARY\")\n",
    "                    else:\n",
    "                        labels.append(\"B-SALARY\")\n",
    "                else:\n",
    "                    labels.append(\"O\")\n",
    "            except:\n",
    "                labels.append(\"O\")\n",
    "\n",
    "        input_dict = {\n",
    "            \"input_ids\": chunk_input_ids.unsqueeze(0).to(device),\n",
    "            \"attention_mask\": attention_mask[i].unsqueeze(0).to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**input_dict)\n",
    "            embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "        tokens_all.append(chunk_tokens)\n",
    "        labels_all.append(labels)\n",
    "        embeddings_all.append(embeddings)\n",
    "\n",
    "    return tokens_all, labels_all, embeddings_all\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, torch.argmax(vec, 1)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def prepare_sequence(embeds):\n",
    "    return embeds.view(len(embeds), 1, -1)\n",
    "\n",
    "# === BiLSTM+CRF Model ===\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tag_to_ix):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        for feat in feats:\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, embeds):\n",
    "        self.hidden = self.init_hidden()\n",
    "        lstm_out, _ = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(embeds), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score += self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score += self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = init_vvars\n",
    "\n",
    "        for feat in feats:\n",
    "            bptrs_t, viterbivars_t = [], []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = torch.argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, embeds, tags):\n",
    "        feats = self._get_lstm_features(embeds)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, embeds):\n",
    "        lstm_feats = self._get_lstm_features(embeds)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "\n",
    "# === Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nation_currency = {\"PH\": \"PHP\", \"NZ\": \"NZD\", \"AUS\": \"AUD\", \"HK\": \"HKD\", \"ID\": \"IDR\", \"MY\": \"MYR\", \"SG\": \"SGD\", \"TH\": \"THB\"}\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device).eval()\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# === Data Preprocessing ===\n",
    "dev_data = pd.read_csv(\"/Users/eddiezhang/Downloads/job_data_files/salary_labelled_development_set.csv\")\n",
    "dev_data['currency'] = dev_data.iloc[:, 3].map(nation_currency)\n",
    "dev_data['parsed'] = dev_data.iloc[:, 5].apply(parse_actual_info)\n",
    "dev_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(dev_data['parsed'].tolist(), index=dev_data.index)\n",
    "dev_data['cleaned_ad_details'] = dev_data['job_ad_details'].astype(str).apply(clean_html_tags).apply(clean_text)\n",
    "\n",
    "# === Build Training Set (with caching) ===\n",
    "tag_to_ix = {label: i for i, label in enumerate(LABELS)}\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    print(\"Loading cached embeddings...\")\n",
    "    X_train, Y_train = torch.load(CACHE_FILE)\n",
    "else:\n",
    "    print(\"Generating embeddings...\")\n",
    "    X_train, Y_train = [], []\n",
    "    for idx, row in dev_data.iterrows():\n",
    "        job_text = row['cleaned_ad_details']\n",
    "        parsed = row['parsed']\n",
    "        if not parsed:\n",
    "            continue\n",
    "        min_salary, max_salary = parsed[0], parsed[1]\n",
    "\n",
    "        token_chunks, label_chunks, embed_chunks = chunk_and_align(job_text, min_salary, max_salary, tokenizer)\n",
    "\n",
    "        for labels, embeddings in zip(label_chunks, embed_chunks):\n",
    "            if len(labels) != embeddings.shape[0]:\n",
    "                continue\n",
    "            X_train.append(embeddings)\n",
    "            Y_train.append(torch.tensor([tag_to_ix[lbl] for lbl in labels], dtype=torch.long))\n",
    "\n",
    "    torch.save((X_train, Y_train), CACHE_FILE)\n",
    "    print(\"Embeddings cached.\")\n",
    "\n",
    "# === Train Model ===\n",
    "model = BiLSTM_CRF(embedding_dim=768, hidden_dim=128, tag_to_ix=tag_to_ix).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0.0\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "        model.zero_grad()\n",
    "        feats = prepare_sequence(x).to(device)\n",
    "        y = y.to(device)\n",
    "        loss = model.neg_log_likelihood(feats, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
