{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === Constants ===\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<END>\"\n",
    "LABELS = [\"B-SALARY\", \"I-SALARY\", \"O\"]\n",
    "\n",
    "# === Helper Functions ===\n",
    "def parse_actual_info(info_str):\n",
    "    parts = info_str.split('-')\n",
    "    if len(parts) != 4 or parts == ['0', '0', 'None', 'None']:\n",
    "        return None\n",
    "    return (float(parts[0]), float(parts[1]), parts[2], parts[3].lower())\n",
    "\n",
    "def clean_html_tags(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    cleaned = re.sub(r'<[^>]+>', '', text)           \n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()   \n",
    "    return cleaned\n",
    "\n",
    "def chunk_and_align(text, min_salary, max_salary, tokenizer, max_length=512, stride=128):\n",
    "    tokens_all, labels_all, embeddings_all = [], [], []\n",
    "\n",
    "    # 1) find all digit‐runs in the text that match either the min or the max\n",
    "    occurrences = []\n",
    "    for m in re.finditer(r'\\d+(?:\\.\\d+)?', text):\n",
    "        v = float(m.group())\n",
    "        if abs(v - min_salary) < 1e-3 or abs(v - max_salary) < 1e-3:\n",
    "            occurrences.append((m.start(), m.end(), v))\n",
    "\n",
    "    # if nothing matched at all, skip this example\n",
    "    if not occurrences:\n",
    "        return tokens_all, labels_all, embeddings_all\n",
    "\n",
    "    # separate out the min‐matches and max‐matches\n",
    "    min_positions = [ (s,e) for s,e,v in occurrences if abs(v - min_salary) < 1e-3 ]\n",
    "    max_positions = [ (s,e) for s,e,v in occurrences if abs(v - max_salary) < 1e-3 ]\n",
    "\n",
    "    # choose start_char\n",
    "    if min_positions:\n",
    "        start_char = min(s for s,e in min_positions)\n",
    "    else:\n",
    "        start_char = min(s for s,e,v in occurrences)\n",
    "\n",
    "    # choose end_char\n",
    "    if max_positions:\n",
    "        end_char = max(e for s,e in max_positions)\n",
    "    else:\n",
    "        end_char = max(e for s,e,v in occurrences)\n",
    "\n",
    "    # 2) now tokenize in overlapping chunks as before\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    offset_mappings = inputs.pop(\"offset_mapping\")\n",
    "    input_ids       = inputs[\"input_ids\"]\n",
    "    attention_mask  = inputs[\"attention_mask\"]\n",
    "\n",
    "    # 3) label each token by whether its span overlaps [start_char, end_char)\n",
    "    for i in range(len(input_ids)):\n",
    "        offsets = offset_mappings[i].tolist()\n",
    "        chunk_ids = input_ids[i]\n",
    "        chunk_tokens = tokenizer.convert_ids_to_tokens(chunk_ids)\n",
    "\n",
    "        labels = []\n",
    "        saw_first = False\n",
    "        for (s, e), token in zip(offsets, chunk_tokens):\n",
    "            if e > start_char and s < end_char:\n",
    "                if not saw_first:\n",
    "                    labels.append(\"B-SALARY\")\n",
    "                    saw_first = True\n",
    "                else:\n",
    "                    labels.append(\"I-SALARY\")\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "\n",
    "        # 4) get embeddings\n",
    "        with torch.no_grad():\n",
    "            out = bert_model(\n",
    "                input_ids=chunk_ids.unsqueeze(0).to(device),\n",
    "                attention_mask=attention_mask[i].unsqueeze(0).to(device),\n",
    "            )\n",
    "        embeddings = out.last_hidden_state.squeeze(0).cpu()\n",
    "\n",
    "        tokens_all.append(chunk_tokens)\n",
    "        labels_all.append(labels)\n",
    "        embeddings_all.append(embeddings)\n",
    "\n",
    "    return tokens_all, labels_all, embeddings_all\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, torch.argmax(vec, 1)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def prepare_sequence(embeds):\n",
    "    return embeds.view(len(embeds), 1, -1)\n",
    "\n",
    "# === BiLSTM+CRF Model ===\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tag_to_ix):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        for feat in feats:\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, embeds):\n",
    "        self.hidden = self.init_hidden()\n",
    "        lstm_out, _ = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(embeds), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score += self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score += self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = init_vvars\n",
    "\n",
    "        for feat in feats:\n",
    "            bptrs_t, viterbivars_t = [], []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = torch.argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, embeds, tags):\n",
    "        feats = self._get_lstm_features(embeds)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, embeds):\n",
    "        lstm_feats = self._get_lstm_features(embeds)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "\n",
    "# === Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nation_currency = {\"PH\": \"PHP\", \"NZ\": \"NZD\", \"AUS\": \"AUD\", \"HK\": \"HKD\", \"ID\": \"IDR\", \"MY\": \"MYR\", \"SG\": \"SGD\", \"TH\": \"THB\"}\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device).eval()\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# === Data Preprocessing ===\n",
    "dev_data = pd.read_csv(\"/Users/eddiezhang/Downloads/job_data_files/salary_labelled_development_set.csv\")\n",
    "dev_data['currency'] = dev_data.iloc[:, 3].map(nation_currency)\n",
    "dev_data['parsed'] = dev_data.iloc[:, 5].apply(parse_actual_info)\n",
    "dev_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(dev_data['parsed'].tolist(), index=dev_data.index)\n",
    "dev_data['cleaned_ad_details'] = dev_data['job_ad_details'].astype(str).apply(clean_html_tags).apply(clean_text)\n",
    "\n",
    "# === Build Training Set ===\n",
    "tag_to_ix = {label: i for i, label in enumerate(LABELS)}\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "\n",
    "for idx, row in dev_data.iterrows():\n",
    "    job_text = row['cleaned_ad_details']\n",
    "    parsed = row['parsed']\n",
    "    if not parsed:\n",
    "        continue\n",
    "    min_salary, max_salary = parsed[0], parsed[1]\n",
    "\n",
    "    token_chunks, label_chunks, embed_chunks = chunk_and_align(job_text, min_salary, max_salary, tokenizer)\n",
    "\n",
    "    for labels, embeddings in zip(label_chunks, embed_chunks):\n",
    "        if len(labels) != embeddings.shape[0]:\n",
    "            continue\n",
    "        X_train.append(embeddings)\n",
    "        Y_train.append(torch.tensor([tag_to_ix[lbl] for lbl in labels], dtype=torch.long))\n",
    "\n",
    "# === Train Model ===\n",
    "model = BiLSTM_CRF(embedding_dim=768, hidden_dim=128, tag_to_ix=tag_to_ix).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0.0\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "        model.zero_grad()\n",
    "        feats = prepare_sequence(x).to(device)\n",
    "        y = y.to(device)\n",
    "        loss = model.neg_log_likelihood(feats, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459eaf1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_actual_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOverall Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Run Evaluation\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m evaluate_on_test_set(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/eddiezhang/Downloads/job_data_files/salary_labelled_test_set.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mevaluate_on_test_set\u001b[0;34m(test_csv_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_on_test_set\u001b[39m(test_csv_path):\n\u001b[1;32m     11\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_csv_path)\n\u001b[0;32m---> 12\u001b[0m     test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(parse_actual_info)\n\u001b[1;32m     13\u001b[0m     test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_ad_details\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     14\u001b[0m         test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_ad_details\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(clean_html_tags)\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     20\u001b[0m     correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_actual_info' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_span_from_tags(tokens, tags):\n",
    "    # find all positions where the model predicted B- or I-SALARY\n",
    "    idxs = [i for i, tag in enumerate(tags) if tag in (\"B-SALARY\", \"I-SALARY\")]\n",
    "    if not idxs:\n",
    "        return \"\"\n",
    "    start, end = min(idxs), max(idxs)\n",
    "    span_tokens = tokens[start : end + 1]\n",
    "    return tokenizer.convert_tokens_to_string(span_tokens).strip()\n",
    "\n",
    "def evaluate_on_test_set(test_csv_path):\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "    test_data['parsed'] = test_data.iloc[:, 5].apply(parse_actual_info)\n",
    "    test_data['cleaned_ad_details'] = (\n",
    "        test_data['job_ad_details']\n",
    "        .astype(str)\n",
    "        .apply(clean_html_tags)\n",
    "        .apply(clean_text)\n",
    "    )\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(\"\\n=== Test Set Evaluation ===\\n\")\n",
    "    for _, row in test_data.iterrows():\n",
    "        job_id = row['job_id']\n",
    "        y_true = row['y_true']\n",
    "        parsed = row['parsed']\n",
    "        if not parsed:\n",
    "            continue\n",
    "        min_salary, max_salary, currency, unit = parsed\n",
    "        job_text = row['cleaned_ad_details']\n",
    "\n",
    "        token_chunks, _, embed_chunks = chunk_and_align(\n",
    "            job_text, min_salary, max_salary, tokenizer\n",
    "        )\n",
    "\n",
    "        best_prediction = \"NONE\"\n",
    "        best_raw_span = \"\"\n",
    "        for tokens, embeddings in zip(token_chunks, embed_chunks):\n",
    "            with torch.no_grad():\n",
    "                feats = prepare_sequence(embeddings).to(device)\n",
    "                _, pred_ids = model(feats)\n",
    "\n",
    "            ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "            pred_tags = [ix_to_tag[i.item()] for i in pred_ids]\n",
    "\n",
    "            raw_span = extract_span_from_tags(tokens, pred_tags)\n",
    "            if not raw_span:\n",
    "                continue\n",
    "\n",
    "            best_raw_span = raw_span\n",
    "\n",
    "            # extract all numbers from the raw span\n",
    "            nums = re.findall(r'\\d+(?:\\.\\d+)?', raw_span)\n",
    "            if len(nums) >= 2:\n",
    "                low, high = nums[0], nums[-1]\n",
    "            elif len(nums) == 1:\n",
    "                low = high = nums[0]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # normalize to integers for formatting\n",
    "            low_i = int(float(low))\n",
    "            high_i = int(float(high))\n",
    "            formatted_range = f\"{low_i}-{high_i}\"\n",
    "\n",
    "            best_prediction = f\"{formatted_range}-{currency.upper()}-{unit.upper()}\"\n",
    "            break\n",
    "\n",
    "        status = \"✅\" if (\n",
    "            best_prediction.replace(\" \", \"\").lower()\n",
    "            in y_true.replace(\" \", \"\").lower()\n",
    "        ) else \"❌\"\n",
    "\n",
    "        print(\n",
    "            f\"[{status}] Job ID {job_id}\\n\"\n",
    "            f\"    Raw span:    '{best_raw_span}'\\n\"\n",
    "            f\"    Formatted:   '{best_prediction}'\\n\"\n",
    "            f\"    Expected:    '{y_true}'\\n\"\n",
    "        )\n",
    "\n",
    "        total += 1\n",
    "        if status == \"✅\":\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\nOverall Accuracy: {correct}/{total} = {accuracy:.2%}\\n\")\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_on_test_set(\"/Users/eddiezhang/Downloads/job_data_files/salary_labelled_test_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eece1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e41cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
