{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === Constants ===\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<END>\"\n",
    "LABELS = [\"B-SALARY\", \"I-SALARY\", \"O\"]\n",
    "\n",
    "# === Helper Functions ===\n",
    "def parse_actual_info(info_str):\n",
    "    parts = info_str.split('-')\n",
    "    if len(parts) != 4 or parts == ['0', '0', 'None', 'None']:\n",
    "        return None\n",
    "    return (float(parts[0]), float(parts[1]), parts[2], parts[3].lower())\n",
    "\n",
    "def clean_html_tags(html_text):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    cleaned = re.sub(r'<[^>]+>', '', text)           \n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()   \n",
    "    return cleaned\n",
    "\n",
    "def chunk_and_align(text, min_salary, max_salary, tokenizer, max_length=512, stride=128):\n",
    "    tokens_all, labels_all, embeddings_all = [], [], []\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    overflow_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mappings = inputs.pop(\"offset_mapping\")\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    for i in range(len(input_ids)):\n",
    "        chunk_offsets = offset_mappings[i].tolist()\n",
    "        chunk_input_ids = input_ids[i]\n",
    "        chunk_tokens = tokenizer.convert_ids_to_tokens(chunk_input_ids)\n",
    "        word_ids = inputs.word_ids(i)\n",
    "\n",
    "        labels = []\n",
    "        for token, offset, word_id in zip(chunk_tokens, chunk_offsets, word_ids):\n",
    "            if word_id is None or offset == [0, 0]:\n",
    "                labels.append(\"O\")\n",
    "                continue\n",
    "\n",
    "            word = text[offset[0]:offset[1]]\n",
    "            value_str = re.sub(r'[^\\d.]', '', word)\n",
    "            if not re.fullmatch(r'\\d+(\\.\\d+)?', value_str):  # e.g. \"123\", \"12.5\"\n",
    "                labels.append(\"O\")\n",
    "                continue\n",
    "\n",
    "            value = float(value_str)\n",
    "\n",
    "            if abs(value - min_salary) < 1e-3:\n",
    "                labels.append(\"B-SALARY\")\n",
    "            elif abs(value - max_salary) < 1e-3:\n",
    "                if labels and labels[-1] == \"B-SALARY\":\n",
    "                    labels.append(\"I-SALARY\")\n",
    "                else:\n",
    "                    labels.append(\"B-SALARY\")\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "\n",
    "        input_dict = {\n",
    "            \"input_ids\": chunk_input_ids.unsqueeze(0).to(device),\n",
    "            \"attention_mask\": attention_mask[i].unsqueeze(0).to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**input_dict)\n",
    "            embeddings = outputs.last_hidden_state.squeeze(0).cpu()\n",
    "\n",
    "        tokens_all.append(chunk_tokens)\n",
    "        labels_all.append(labels)\n",
    "        embeddings_all.append(embeddings)\n",
    "\n",
    "    return tokens_all, labels_all, embeddings_all\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, torch.argmax(vec, 1)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "def prepare_sequence(embeds):\n",
    "    return embeds.view(len(embeds), 1, -1)\n",
    "\n",
    "# === BiLSTM+CRF Model ===\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tag_to_ix):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        for feat in feats:\n",
    "            alphas_t = []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, embeds):\n",
    "        self.hidden = self.init_hidden()\n",
    "        lstm_out, _ = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(embeds), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score += self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score += self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        forward_var = init_vvars\n",
    "\n",
    "        for feat in feats:\n",
    "            bptrs_t, viterbivars_t = [], []\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = torch.argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, embeds, tags):\n",
    "        feats = self._get_lstm_features(embeds)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, embeds):\n",
    "        lstm_feats = self._get_lstm_features(embeds)\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq\n",
    "\n",
    "# === Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nation_currency = {\"PH\": \"PHP\", \"NZ\": \"NZD\", \"AUS\": \"AUD\", \"HK\": \"HKD\", \"ID\": \"IDR\", \"MY\": \"MYR\", \"SG\": \"SGD\", \"TH\": \"THB\"}\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-multilingual-cased\").to(device).eval()\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# === Data Preprocessing ===\n",
    "dev_data = pd.read_csv(\"/Users/eddiezhang/Downloads/job_data_files/salary_labelled_development_set.csv\")\n",
    "dev_data['currency'] = dev_data.iloc[:, 3].map(nation_currency)\n",
    "dev_data['parsed'] = dev_data.iloc[:, 5].apply(parse_actual_info)\n",
    "dev_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(dev_data['parsed'].tolist(), index=dev_data.index)\n",
    "dev_data['cleaned_ad_details'] = dev_data['job_ad_details'].astype(str).apply(clean_html_tags).apply(clean_text)\n",
    "\n",
    "# === Build Training Set ===\n",
    "tag_to_ix = {label: i for i, label in enumerate(LABELS)}\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n",
    "\n",
    "X_train, Y_train = [], []\n",
    "\n",
    "for idx, row in dev_data.iterrows():\n",
    "    job_text = row['cleaned_ad_details']\n",
    "    parsed = row['parsed']\n",
    "    if not parsed:\n",
    "        continue\n",
    "    min_salary, max_salary = parsed[0], parsed[1]\n",
    "\n",
    "    token_chunks, label_chunks, embed_chunks = chunk_and_align(job_text, min_salary, max_salary, tokenizer)\n",
    "\n",
    "    for labels, embeddings in zip(label_chunks, embed_chunks):\n",
    "        if len(labels) != embeddings.shape[0]:\n",
    "            continue\n",
    "        X_train.append(embeddings)\n",
    "        Y_train.append(torch.tensor([tag_to_ix[lbl] for lbl in labels], dtype=torch.long))\n",
    "\n",
    "# === Train Model ===\n",
    "model = BiLSTM_CRF(embedding_dim=768, hidden_dim=128, tag_to_ix=tag_to_ix).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0.0\n",
    "    for x, y in zip(X_train, Y_train):\n",
    "        model.zero_grad()\n",
    "        feats = prepare_sequence(x).to(device)\n",
    "        y = y.to(device)\n",
    "        loss = model.neg_log_likelihood(feats, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459eaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_span_from_tags(tokens, tags):\n",
    "    span_tokens = []\n",
    "    inside = False\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag == \"B-SALARY\":\n",
    "            span_tokens = [token]\n",
    "            inside = True\n",
    "        elif tag == \"I-SALARY\" and inside:\n",
    "            span_tokens.append(token)\n",
    "        elif inside:\n",
    "            break\n",
    "    return tokenizer.convert_tokens_to_string(span_tokens).strip()\n",
    "\n",
    "def evaluate_on_test_set(test_csv_path):\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "    test_data['parsed'] = test_data.iloc[:, 5].apply(parse_actual_info)\n",
    "    test_data['cleaned_ad_details'] = test_data['job_ad_details'].astype(str).apply(clean_html_tags).apply(clean_text)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(\"\\n=== Test Set Evaluation ===\\n\")\n",
    "    for idx, row in test_data.iterrows():\n",
    "        job_id = row['job_id']\n",
    "        job_text = row['cleaned_ad_details']\n",
    "        y_true = row['y_true']\n",
    "        parsed = row['parsed']\n",
    "        if not parsed:\n",
    "            continue\n",
    "\n",
    "        min_salary, max_salary = parsed[0], parsed[1]\n",
    "        token_chunks, _, embed_chunks = chunk_and_align(job_text, min_salary, max_salary, tokenizer)\n",
    "\n",
    "        best_prediction = \"NONE\"\n",
    "        for tokens, embeddings in zip(token_chunks, embed_chunks):\n",
    "            with torch.no_grad():\n",
    "                feats = prepare_sequence(embeddings).to(device)\n",
    "                _, pred_ids = model(feats)\n",
    "            ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "            pred_tags = [ix_to_tag[i.item()] for i in pred_ids]\n",
    "            pred_span = extract_span_from_tags(tokens, pred_tags)\n",
    "            if pred_span:\n",
    "                y_parts = y_true.split('-')\n",
    "                currency_unit = f\"{y_parts[2]}-{y_parts[3]}\" if len(y_parts) == 4 else \"UNKNOWN-UNKNOWN\"\n",
    "                best_prediction = f\"{pred_span}-{currency_unit}\"\n",
    "                break\n",
    "\n",
    "        match = (best_prediction.replace(\" \", \"\").lower() in y_true.replace(\" \", \"\").lower())\n",
    "        status = \"✅\" if match else \"❌\"\n",
    "        print(f\"[{status}] Job ID {job_id} | Predicted: '{best_prediction}' | Expected: '{y_true}'\")\n",
    "\n",
    "        total += 1\n",
    "        if match:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f\"\\nAccuracy: {correct}/{total} = {accuracy:.2%}\\n\")\n",
    "\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_on_test_set(\"/Users/eddiezhang/Downloads/job_data_files/salary_labelled_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# from transformers import BertTokenizerFast, BertModel\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # === Helper Functions === \n",
    "# def parse_actual_info(info_str):\n",
    "#     parts = info_str.split('-')\n",
    "#     if len(parts) != 4 or parts == ['0', '0', 'None', 'None']:\n",
    "#         return None\n",
    "#     return (float(parts[0]), float(parts[1]), parts[2], parts[3].lower())\n",
    "\n",
    "# # Removes html tags using BeautifulSoup\n",
    "# def clean_html_tags(html_text):\n",
    "#     soup = BeautifulSoup(html_text, 'html.parser')\n",
    "#     return soup.get_text()\n",
    "\n",
    "# # Cleans text (removing html tags that may have been missed, reducing whitespaces)\n",
    "# def clean_text(text):\n",
    "#     if pd.isna(text):\n",
    "#         return \"\"\n",
    "#     cleaned = re.sub(r'<[^>]+>', '', text)           \n",
    "#     cleaned = re.sub(r'\\s+', ' ', cleaned).strip()   \n",
    "#     return cleaned\n",
    "\n",
    "# def get_aligned_tokens_and_labels(text, min_salary, max_salary):\n",
    "#     inputs = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=4096, return_tensors=\"pt\")\n",
    "#     offsets = inputs['offset_mapping'][0].tolist()\n",
    "#     input_ids = inputs['input_ids'][0]\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#     word_ids = inputs.word_ids()\n",
    "    \n",
    "#     labels = []\n",
    "#     for token, offset, word_id in zip(tokens, offsets, word_ids):\n",
    "#         if word_id is None or offset == [0, 0]:\n",
    "#             labels.append(\"O\")\n",
    "#             continue\n",
    "#         word = text[offset[0]:offset[1]]\n",
    "#         try:\n",
    "#             value = float(re.sub(r'[^\\d.]', '', word))\n",
    "#             if min_salary <= value <= max_salary:\n",
    "#                 if labels and labels[-1] in [\"B-SALARY\", \"I-SALARY\"]:\n",
    "#                     labels.append(\"I-SALARY\")\n",
    "#                 else:\n",
    "#                     labels.append(\"B-SALARY\")\n",
    "#             else:\n",
    "#                 labels.append(\"O\")\n",
    "#         except:\n",
    "#             labels.append(\"O\")\n",
    "#     return tokens, labels, inputs\n",
    "\n",
    "# def get_token_embeddings_from_inputs(inputs, model, device):\n",
    "#     \"\"\"\n",
    "#     Returns token-level contextual embeddings from a model given tokenizer outputs.\n",
    "#     Strips keys that the model does not use (e.g., offset_mapping).\n",
    "#     \"\"\"\n",
    "#     inputs_model = {k: v.to(device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs_model)\n",
    "#         embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "#     return embeddings\n",
    "\n",
    "# def extract_span(tokens, labels):\n",
    "#     span_tokens = []\n",
    "#     inside = False\n",
    "#     for token, label in zip(tokens, labels):\n",
    "#         if label == \"B-SALARY\":\n",
    "#             span_tokens = [token]\n",
    "#             inside = True\n",
    "#         elif label == \"I-SALARY\" and inside:\n",
    "#             span_tokens.append(token)\n",
    "#         elif inside:\n",
    "#             break\n",
    "#     return tokenizer.convert_tokens_to_string(span_tokens)\n",
    "\n",
    "# # === Setup ===\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# nation_currency = {\n",
    "#     \"PH\": \"PHP\", \n",
    "#     \"NZ\": \"NZD\", \n",
    "#     \"AUS\": \"AUD\", \n",
    "#     \"HK\": \"HKD\",\n",
    "#     \"ID\": \"IDR\", \n",
    "#     \"MY\": \"MYR\", \n",
    "#     \"SG\": \"SGD\", \n",
    "#     \"TH\": \"THB\"\n",
    "# }\n",
    "\n",
    "# # Load multilingual BERT model and tokenizer\n",
    "# model_name = \"bert-base-multilingual-cased\"\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "# model.eval()\n",
    "\n",
    "# class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "#     def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "#         super(BiLSTM_CRF, self).__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.tag_to_ix = tag_to_ix \n",
    "#         self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "#         self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "#                             num_layers=1, bidirectional=True)\n",
    "\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size) #TODO: Challenge: Why is this layer required? (DONE)\n",
    "#         # Layer is required to transform the hidden states into a space where each dimension corresponds to a particular tag's emission score\n",
    "\n",
    "#         # Matrix of transition parameters.  Entry i,j is the score of\n",
    "#         # transitioning *to* i *from* j.\n",
    "#         self.transitions = nn.Parameter(\n",
    "#             torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "#         self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "#         self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "#     def init_hidden(self):\n",
    "#         return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "#                 torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "#     def _forward_alg(self, feats):\n",
    "#         # Do the forward algorithm to compute the partition function\n",
    "#         init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "#         # START_TAG has all of the score.\n",
    "#         init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "#         # Wrap in a variable so that we will get automatic backprop\n",
    "#         forward_var = init_alphas\n",
    "\n",
    "#         # Iterate through the sentence\n",
    "#         for feat in feats:\n",
    "#             alphas_t = []  # The forward tensors at this timestep\n",
    "#             for next_tag in range(self.tagset_size):\n",
    "#                 # broadcast the emission score: it is the same regardless of\n",
    "#                 # the previous tag\n",
    "#                 emit_score = feat[next_tag].view(\n",
    "#                     1, -1).expand(1, self.tagset_size)\n",
    "#                 # the ith entry of trans_score is the score of transitioning to\n",
    "#                 # next_tag from i\n",
    "#                 trans_score = self.transitions[next_tag].view(1, -1)\n",
    "#                 # The ith entry of next_tag_var is the value for the\n",
    "#                 # edge (i -> next_tag) before we do log-sum-exp\n",
    "#                 next_tag_var = forward_var + trans_score + emit_score\n",
    "#                 # The forward variable for this tag is log-sum-exp of all the\n",
    "#                 # scores.\n",
    "#                 alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "#             forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "#         terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "#         alpha = log_sum_exp(terminal_var)\n",
    "#         return alpha\n",
    "\n",
    "#     def _get_lstm_features(self, sentence):\n",
    "#         self.hidden = self.init_hidden()\n",
    "#         embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "#         lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "#         lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "#         lstm_feats = self.hidden2tag(lstm_out) \n",
    "#         return lstm_feats\n",
    "\n",
    "#     def _score_sentence(self, feats, tags):\n",
    "#         # Gives the score of a provided tag sequence\n",
    "#         score = torch.zeros(1)\n",
    "#         tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "#         for i, feat in enumerate(feats):\n",
    "#             score = score + \\\n",
    "#                 self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "#         score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "#         return score\n",
    "\n",
    "#     def _viterbi_decode(self, feats):\n",
    "#         backpointers = []\n",
    "\n",
    "#         # Initialize the viterbi variables in log space\n",
    "#         init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "#         init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "#         # forward_var at step i holds the viterbi variables for step i-1\n",
    "#         forward_var = init_vvars\n",
    "#         for feat in feats:\n",
    "#             bptrs_t = []  # holds the backpointers for this step\n",
    "#             viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "#             for next_tag in range(self.tagset_size):\n",
    "#                 # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "#                 # previous step, plus the score of transitioning\n",
    "#                 # from tag i to next_tag.\n",
    "#                 # We don't include the emission scores here because the max\n",
    "#                 # does not depend on them (we add them in below)\n",
    "#                 next_tag_var = forward_var + self.transitions[next_tag]\n",
    "#                 best_tag_id = argmax(next_tag_var)\n",
    "#                 bptrs_t.append(best_tag_id)\n",
    "#                 viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "#             # Now add in the emission scores, and assign forward_var to the set\n",
    "#             # of viterbi variables we just computed\n",
    "#             forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "#             backpointers.append(bptrs_t)\n",
    "\n",
    "#         # Transition to STOP_TAG\n",
    "#         terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "#         best_tag_id = argmax(terminal_var)\n",
    "#         path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "#         # Follow the back pointers to decode the best path.\n",
    "#         best_path = [best_tag_id]\n",
    "#         for bptrs_t in reversed(backpointers):\n",
    "#             best_tag_id = bptrs_t[best_tag_id]\n",
    "#             best_path.append(best_tag_id)\n",
    "#         # Pop off the start tag (we dont want to return that to the caller)\n",
    "#         start = best_path.pop()\n",
    "#         assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "#         best_path.reverse()\n",
    "#         return path_score, best_path\n",
    "\n",
    "#     def neg_log_likelihood(self, sentence, tags):\n",
    "#         feats = self._get_lstm_features(sentence)\n",
    "#         forward_score = self._forward_alg(feats)\n",
    "#         gold_score = self._score_sentence(feats, tags)\n",
    "#         return forward_score - gold_score\n",
    "\n",
    "#     def forward(self, sentence):  # dont confuse this with _forward_alg above.       \n",
    "#         lstm_feats = self._get_lstm_features(sentence)\n",
    "#         score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "#         return score, tag_seq\n",
    "\n",
    "# # === Preprocessing ===\n",
    "\n",
    "# dev_data = pd.read_csv('/Users/eddiezhang/Downloads/job_data_files/salary_labelled_development_set.csv')\n",
    "\n",
    "# # Parse expected salary value for easier comparison\n",
    "# dev_data['currency'] = dev_data.iloc[:, 3].map(nation_currency)\n",
    "# dev_data['parsed'] = dev_data.iloc[:, 5].apply(parse_actual_info)\n",
    "# dev_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(dev_data['parsed'].tolist(), index=dev_data.index)\n",
    "\n",
    "# # Cleaning job ad details to remove html tags and additional whitespaces\n",
    "# dev_data['cleaned_ad_details'] = dev_data['job_ad_details'].astype(str).apply(clean_html_tags).apply(clean_text)\n",
    "\n",
    "# # === Training ===\n",
    "\n",
    "# train_embeddings, train_labels = [], []\n",
    "\n",
    "# for idx, row in dev_data.iterrows():\n",
    "#     job_text = row.iloc[2]\n",
    "#     min_salary = row['min_salary']\n",
    "#     max_salary = row['max_salary']\n",
    "\n",
    "#     tokens, labels, inputs = get_aligned_tokens_and_labels(job_text, min_salary, max_salary)\n",
    "\n",
    "#     if \"B-SALARY\" not in labels:\n",
    "#         continue\n",
    "\n",
    "#     embeddings = get_token_embeddings_from_inputs(inputs, model, device)\n",
    "\n",
    "#     if len(labels) != embeddings.shape[0]:\n",
    "#         print(f\"[SKIP {idx}] Mismatch: {len(labels)} labels vs {embeddings.shape[0]} embeddings\")\n",
    "#         continue\n",
    "\n",
    "#     train_embeddings.extend(embeddings.numpy())\n",
    "#     train_labels.extend(labels)\n",
    "\n",
    "# train_embeddings = np.array(train_embeddings)\n",
    "# label_encoder = LabelEncoder()\n",
    "# encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "# clf = LogisticRegression(max_iter=1000)\n",
    "# clf.fit(train_embeddings, encoded_train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import re\n",
    "# from transformers import LongformerTokenizerFast, LongformerModel\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # === Helper Functions ===\n",
    "\n",
    "# def get_aligned_tokens_and_labels(text, min_salary, max_salary):\n",
    "#     inputs = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=4096, return_tensors=\"pt\")\n",
    "#     offsets = inputs['offset_mapping'][0].tolist()\n",
    "#     input_ids = inputs['input_ids'][0]\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#     word_ids = inputs.word_ids()\n",
    "    \n",
    "#     labels = []\n",
    "#     for token, offset, word_id in zip(tokens, offsets, word_ids):\n",
    "#         if word_id is None or offset == [0, 0]:\n",
    "#             labels.append(\"O\")\n",
    "#             continue\n",
    "#         word = text[offset[0]:offset[1]]\n",
    "#         try:\n",
    "#             value = float(re.sub(r'[^\\d.]', '', word))\n",
    "#             if min_salary <= value <= max_salary:\n",
    "#                 if labels and labels[-1] in [\"B-SALARY\", \"I-SALARY\"]:\n",
    "#                     labels.append(\"I-SALARY\")\n",
    "#                 else:\n",
    "#                     labels.append(\"B-SALARY\")\n",
    "#             else:\n",
    "#                 labels.append(\"O\")\n",
    "#         except:\n",
    "#             labels.append(\"O\")\n",
    "#     return tokens, labels, inputs\n",
    "\n",
    "# def get_token_embeddings_from_inputs(inputs, model, device):\n",
    "#     \"\"\"\n",
    "#     Returns token-level contextual embeddings from a model given tokenizer outputs.\n",
    "#     Strips keys that the model does not use (e.g., offset_mapping).\n",
    "#     \"\"\"\n",
    "#     inputs_model = {k: v.to(device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs_model)\n",
    "#         embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "#     return embeddings\n",
    "\n",
    "# def extract_span(tokens, labels):\n",
    "#     span_tokens = []\n",
    "#     inside = False\n",
    "#     for token, label in zip(tokens, labels):\n",
    "#         if label == \"B-SALARY\":\n",
    "#             span_tokens = [token]\n",
    "#             inside = True\n",
    "#         elif label == \"I-SALARY\" and inside:\n",
    "#             span_tokens.append(token)\n",
    "#         elif inside:\n",
    "#             break\n",
    "#     return tokenizer.convert_tokens_to_string(span_tokens)\n",
    "\n",
    "# # === Device setup ===\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# # === Load datasets ===\n",
    "# dev_data = pd.read_csv('/Users/eddiezhang/Downloads/job_data_files/salary_labelled_development_set.csv')\n",
    "# test_data = pd.read_csv('/Users/eddiezhang/Downloads/job_data_files/salary_labelled_test_set.csv')\n",
    "\n",
    "# # === Currency map ===\n",
    "# nation_currency = {\n",
    "#     \"PH\": \"PHP\", \"NZ\": \"NZD\", \"AUS\": \"AUD\", \"HK\": \"HKD\",\n",
    "#     \"ID\": \"IDR\", \"MY\": \"MYR\", \"SG\": \"SGD\", \"TH\": \"THB\"\n",
    "# }\n",
    "# dev_data['currency'] = dev_data.iloc[:, 3].map(nation_currency)\n",
    "# test_data['currency'] = test_data.iloc[:, 3].map(nation_currency)\n",
    "\n",
    "# # === Parse & clean ===\n",
    "# dev_data['parsed'] = dev_data.iloc[:, 5].apply(parse_actual_info)\n",
    "# test_data['parsed'] = test_data.iloc[:, 5].apply(parse_actual_info)\n",
    "# dev_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(dev_data['parsed'].tolist(), index=dev_data.index)\n",
    "# test_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(test_data['parsed'].tolist(), index=test_data.index)\n",
    "\n",
    "# # === Load Longformer ===\n",
    "# tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "# model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # === Training ===\n",
    "# train_embeddings, train_labels = [], []\n",
    "\n",
    "# for idx, row in dev_data.iterrows():\n",
    "#     job_text = row.iloc[2]\n",
    "#     min_salary = row['min_salary']\n",
    "#     max_salary = row['max_salary']\n",
    "\n",
    "#     tokens, labels, inputs = get_aligned_tokens_and_labels(job_text, min_salary, max_salary)\n",
    "\n",
    "#     if \"B-SALARY\" not in labels:\n",
    "#         continue\n",
    "\n",
    "#     embeddings = get_token_embeddings_from_inputs(inputs, model, device)\n",
    "\n",
    "#     if len(labels) != embeddings.shape[0]:\n",
    "#         print(f\"[SKIP {idx}] Mismatch: {len(labels)} labels vs {embeddings.shape[0]} embeddings\")\n",
    "#         continue\n",
    "\n",
    "#     train_embeddings.extend(embeddings.numpy())\n",
    "#     train_labels.extend(labels)\n",
    "\n",
    "# train_embeddings = np.array(train_embeddings)\n",
    "# label_encoder = LabelEncoder()\n",
    "# encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "# clf = LogisticRegression(max_iter=1000)\n",
    "# clf.fit(train_embeddings, encoded_train_labels)\n",
    "\n",
    "# # === Testing ===\n",
    "# test_embeddings, test_labels = [], []\n",
    "\n",
    "# for idx, row in test_data.iterrows():\n",
    "#     job_text = row.iloc[2]\n",
    "#     min_salary = row['min_salary']\n",
    "#     max_salary = row['max_salary']\n",
    "\n",
    "#     tokens, labels, inputs = get_aligned_tokens_and_labels(job_text, min_salary, max_salary)\n",
    "\n",
    "#     if \"B-SALARY\" not in labels:\n",
    "#         continue\n",
    "\n",
    "#     embeddings = get_token_embeddings_from_inputs(inputs, model, device)\n",
    "\n",
    "#     if len(labels) != embeddings.shape[0]:\n",
    "#         continue\n",
    "\n",
    "#     test_embeddings.extend(embeddings.numpy())\n",
    "#     test_labels.extend(labels)\n",
    "\n",
    "# test_embeddings = np.array(test_embeddings)\n",
    "# encoded_test_labels = label_encoder.transform(test_labels)\n",
    "# test_preds = clf.predict(test_embeddings)\n",
    "\n",
    "# # === Evaluation ===\n",
    "# print(classification_report(encoded_test_labels, test_preds, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
