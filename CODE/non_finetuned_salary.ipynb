{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462eece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import LongformerTokenizerFast, LongformerModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === Helper Functions ===\n",
    "\n",
    "def parse_actual_info(info_str):\n",
    "    parts = info_str.split('-')\n",
    "    if len(parts) != 4 or parts == ['0', '0', 'None', 'None']:\n",
    "        return None\n",
    "    return (float(parts[0]), float(parts[1]), parts[2], parts[3].lower())\n",
    "\n",
    "def get_aligned_tokens_and_labels(text, min_salary, max_salary):\n",
    "    inputs = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=4096, return_tensors=\"pt\")\n",
    "    offsets = inputs['offset_mapping'][0].tolist()\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    word_ids = inputs.word_ids()\n",
    "    \n",
    "    labels = []\n",
    "    for token, offset, word_id in zip(tokens, offsets, word_ids):\n",
    "        if word_id is None or offset == [0, 0]:\n",
    "            labels.append(\"O\")\n",
    "            continue\n",
    "        word = text[offset[0]:offset[1]]\n",
    "        try:\n",
    "            value = float(re.sub(r'[^\\d.]', '', word))\n",
    "            if min_salary <= value <= max_salary:\n",
    "                if labels and labels[-1] in [\"B-SALARY\", \"I-SALARY\"]:\n",
    "                    labels.append(\"I-SALARY\")\n",
    "                else:\n",
    "                    labels.append(\"B-SALARY\")\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "        except:\n",
    "            labels.append(\"O\")\n",
    "    return tokens, labels, inputs\n",
    "\n",
    "def get_token_embeddings_from_inputs(inputs, model, device):\n",
    "    \"\"\"\n",
    "    Returns token-level contextual embeddings from a model given tokenizer outputs.\n",
    "    Strips keys that the model does not use (e.g., offset_mapping).\n",
    "    \"\"\"\n",
    "    inputs_model = {k: v.to(device) for k, v in inputs.items() if k != 'offset_mapping'}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs_model)\n",
    "        embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "    return embeddings\n",
    "\n",
    "def extract_span(tokens, labels):\n",
    "    span_tokens = []\n",
    "    inside = False\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label == \"B-SALARY\":\n",
    "            span_tokens = [token]\n",
    "            inside = True\n",
    "        elif label == \"I-SALARY\" and inside:\n",
    "            span_tokens.append(token)\n",
    "        elif inside:\n",
    "            break\n",
    "    return tokenizer.convert_tokens_to_string(span_tokens)\n",
    "\n",
    "# === Device setup ===\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# === Load datasets ===\n",
    "dev_data = pd.read_csv('/Users/eddiezhang/Downloads/job_data_files/salary_labelled_development_set.csv')\n",
    "test_data = pd.read_csv('/Users/eddiezhang/Downloads/job_data_files/salary_labelled_test_set.csv')\n",
    "\n",
    "# === Currency map ===\n",
    "nation_currency = {\n",
    "    \"PH\": \"PHP\", \"NZ\": \"NZD\", \"AUS\": \"AUD\", \"HK\": \"HKD\",\n",
    "    \"ID\": \"IDR\", \"MY\": \"MYR\", \"SG\": \"SGD\", \"TH\": \"THB\"\n",
    "}\n",
    "dev_data['currency'] = dev_data.iloc[:, 3].map(nation_currency)\n",
    "test_data['currency'] = test_data.iloc[:, 3].map(nation_currency)\n",
    "\n",
    "# === Parse & clean ===\n",
    "dev_data['parsed'] = dev_data.iloc[:, 5].apply(parse_actual_info)\n",
    "test_data['parsed'] = test_data.iloc[:, 5].apply(parse_actual_info)\n",
    "dev_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(dev_data['parsed'].tolist(), index=dev_data.index)\n",
    "test_data[['min_salary', 'max_salary', 'currency', 'unit']] = pd.DataFrame(test_data['parsed'].tolist(), index=test_data.index)\n",
    "\n",
    "# === Load Longformer ===\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096')\n",
    "model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# === Training ===\n",
    "train_embeddings, train_labels = [], []\n",
    "\n",
    "for idx, row in dev_data.iterrows():\n",
    "    job_text = row.iloc[2]\n",
    "    min_salary = row['min_salary']\n",
    "    max_salary = row['max_salary']\n",
    "\n",
    "    tokens, labels, inputs = get_aligned_tokens_and_labels(job_text, min_salary, max_salary)\n",
    "\n",
    "    if \"B-SALARY\" not in labels:\n",
    "        continue\n",
    "\n",
    "    embeddings = get_token_embeddings_from_inputs(inputs, model, device)\n",
    "\n",
    "    if len(labels) != embeddings.shape[0]:\n",
    "        print(f\"[SKIP {idx}] Mismatch: {len(labels)} labels vs {embeddings.shape[0]} embeddings\")\n",
    "        continue\n",
    "\n",
    "    train_embeddings.extend(embeddings.numpy())\n",
    "    train_labels.extend(labels)\n",
    "\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_train_labels = label_encoder.fit_transform(train_labels)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_embeddings, encoded_train_labels)\n",
    "\n",
    "# === Testing ===\n",
    "test_embeddings, test_labels = [], []\n",
    "\n",
    "for idx, row in test_data.iterrows():\n",
    "    job_text = row.iloc[2]\n",
    "    min_salary = row['min_salary']\n",
    "    max_salary = row['max_salary']\n",
    "\n",
    "    tokens, labels, inputs = get_aligned_tokens_and_labels(job_text, min_salary, max_salary)\n",
    "\n",
    "    if \"B-SALARY\" not in labels:\n",
    "        continue\n",
    "\n",
    "    embeddings = get_token_embeddings_from_inputs(inputs, model, device)\n",
    "\n",
    "    if len(labels) != embeddings.shape[0]:\n",
    "        continue\n",
    "\n",
    "    test_embeddings.extend(embeddings.numpy())\n",
    "    test_labels.extend(labels)\n",
    "\n",
    "test_embeddings = np.array(test_embeddings)\n",
    "encoded_test_labels = label_encoder.transform(test_labels)\n",
    "test_preds = clf.predict(test_embeddings)\n",
    "\n",
    "# === Evaluation ===\n",
    "print(classification_report(encoded_test_labels, test_preds, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
